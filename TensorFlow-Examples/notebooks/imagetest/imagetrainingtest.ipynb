{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load a data set\n",
    "#https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/\n",
    "# see\n",
    "# \n",
    "#http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "# Image Parameters\n",
    "N_CLASSES = 2 # CHANGE HERE, total number of classes\n",
    "IMG_HEIGHT = 32 # CHANGE HERE, the image height to be resized to\n",
    "IMG_WIDTH = 32 # CHANGE HERE, the image width to be resized to\n",
    "CHANNELS = 3 # The 3 color channels, change to 1 if grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#images are contained in data dictionaries extracted using the following files\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract each of the training data sets\n",
    "data1=unpickle('../../../../data/cifar-10-batches-py/data_batch_1')\n",
    "data2=unpickle('../../../../data/cifar-10-batches-py/data_batch_2')\n",
    "data3=unpickle('../../../../data/cifar-10-batches-py/data_batch_3')\n",
    "data4=unpickle('../../../../data/cifar-10-batches-py/data_batch_4')\n",
    "data5=unpickle('../../../../data/cifar-10-batches-py/data_batch_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'batch_label'\n",
      "b'labels'\n",
      "b'data'\n",
      "b'filenames'\n"
     ]
    }
   ],
   "source": [
    "#print each of the keys for a given dictionary\n",
    "\n",
    "for x in data1:\n",
    "        print(x)\n",
    "        #print(data1[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'camion_s_000148.png'\n"
     ]
    }
   ],
   "source": [
    "#print filenames\n",
    "filenames=data1[b'filenames']\n",
    "print(filenames[1])\n",
    "label1=data1[b'labels']\n",
    "idata1=data1[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "image=idata1[1]\n",
    "image=image.reshape(32,32,3)\n",
    "img_pil = array_to_img(image)\n",
    "print(type(img_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#print(idata1.shape)\n",
    "print((label1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=1\n",
    "batch_size=10000\n",
    "\n",
    "for image in idata1:\n",
    "        #print(index)\n",
    "        index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index=1\n",
    "batch_size=10000\n",
    "for image in idata1:\n",
    "        #print(index)\n",
    "        if index%100 == 0:\n",
    "            print(index)\n",
    "        #print(x)\n",
    "        #print(data1[x])\n",
    "        label=label1[index-1]\n",
    "        #print((image.size()))\n",
    "        image=image.reshape(32,32,3)\n",
    "        #img_pil = array_to_img(image)\n",
    "        # Resize images to a common size\n",
    "        image1 = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "        # Normalize\n",
    "        #image1 = image1 * 1.0/127.5 - 1.0\n",
    "        #index=index+1\n",
    "        # Create batches\n",
    "        X, Y = tf.train.batch([image1, label], batch_size=batch_size,\n",
    "                              capacity=batch_size * 8,\n",
    "                              num_threads=4)\n",
    "        index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
